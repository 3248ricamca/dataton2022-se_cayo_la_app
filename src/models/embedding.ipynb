{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk import download\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "from gensim.models import Word2Vec\n",
    "from curses.ascii import isalpha\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Para calcular similitud de vectores de misma longitud\n",
    "from gensim.matutils import unitvec\n",
    "\n",
    "# Stemmer de palabras\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowballstemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Categorias relevantes\n",
    "variables = [\"macroeconomia\",\"sostenibilidad\",\"regulaciones\",\"reputacion\",\"alianzas\",\"innovacion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar url para leer los datos de otro subfolder con path relativo\n",
    "# os.chdir('../')\n",
    "\n",
    "# Descarga de archivos nltk necesarios para modelar, descargue todo\n",
    "# download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = pd.read_csv('../datos/noticias.csv')\n",
    "clientes_noticias = pd.read_csv('../datos/clientes_noticias.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_news(df):\n",
    "    df['len_titular'] = df['news_title'].apply(lambda x: len(x))\n",
    "    df['len_content'] = df['news_text_content'].apply(lambda x: len(x))\n",
    "    return df\n",
    "    \n",
    "def create_tokenization(x):\n",
    "    data = []\n",
    "    for j in word_tokenize(x,language='spanish'):\n",
    "        term = j.lower()\n",
    "\n",
    "        if term.isalpha() and not term in spanish_stop_words:\n",
    "            #term = snowballstemmer.stem(term)\n",
    "            data.append(term)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def document_vector(word2vec_model, doc, vocab):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in vocab]\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(vocab, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in vocab for word in doc)\n",
    "\n",
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, condition_on_doc,vocab):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if has_vector_representation(vocab,doc)]\n",
    "\n",
    "    corpus = [doc for doc in corpus if has_vector_representation(vocab,doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def similitud(v1,v2):\n",
    "    \"\"\"\n",
    "        Esta funcion calcula la similitud de dos vectores de misma longitud\n",
    "\n",
    "        Inputs:\n",
    "            v1: vector 1\n",
    "            v2: vector 2\n",
    "        \n",
    "        Output: float\n",
    "\n",
    "            Similitud de vectores\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dot(unitvec(v1), unitvec(v2))\n",
    "\n",
    "def df_categoria_noticia(df_noticas,lista_temas,modelo,vector_promedio_noticias):\n",
    "\n",
    "    diccionario_resultados = {}\n",
    "    for variable in lista_temas:\n",
    "        vector_tema = modelo.wv[variable]\n",
    "        similitud_noticias = [similitud(a,vector_tema) for a in vector_promedio_noticias]\n",
    "        diccionario_resultados[variable] = similitud_noticias\n",
    "\n",
    "    resultados_temas = pd.DataFrame(diccionario_resultados)\n",
    "    resultados_temas['Categoria'] = resultados_temas.idxmax(axis=1)\n",
    "    resultados_temas['Similitud'] = resultados_temas.max(axis=1)\n",
    "\n",
    "    # df_salida = pd.concat([df_noticas,resultados_temas[['Categoria']]],axis=1)\n",
    "    df_salida = pd.concat([df_noticas,resultados_temas[['Categoria','Similitud']]],axis=1)\n",
    "\n",
    "    return df_salida\n",
    "\n",
    "def preprocessing_noticias(df):\n",
    "\n",
    "    print(df.shape)\n",
    "    df = df[df['news_text_content']!=' '].reset_index(drop=True).copy()\n",
    "    print(df.shape)\n",
    "    df = df.drop_duplicates()\n",
    "    print(df.shape)\n",
    "    data = df['news_text_content'].apply(create_tokenization)\n",
    "    # data_titulas = df['news_title'].apply(create_tokenization)\n",
    "    \n",
    "    return [df, data]\n",
    "\n",
    "def matrix_by_new(df,model, vocab, vector_size, output_name = None, run = False, save = False):\n",
    "\n",
    "    if run:\n",
    "        new_mean_vector = []\n",
    "        for doc in df: # append the vector for each document\n",
    "            try:\n",
    "                output = document_vector(model, doc, vocab=vocab)\n",
    "            except:\n",
    "                output = np.zeros(vector_size)\n",
    "            new_mean_vector.append(output)\n",
    "\n",
    "        mean_vector = np.array(new_mean_vector) # list to array\n",
    "        if save:\n",
    "            with open(output_name+'.npy', 'wb') as f:\n",
    "                np.save(f, new_mean_vector)\n",
    "\n",
    "    else:\n",
    "        mean_vector = np.load('array_mean_vector_model.npy')\n",
    "        \n",
    "    return mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23377, 6)\n",
      "(23346, 6)\n",
      "(23346, 6)\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing_noticias(noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time  # To time our operations\n",
    "\n",
    "run_model = False\n",
    "path_model = \"models/Model/word2vec.model\"\n",
    "# semilla = 2022\n",
    "vector_size = 100\n",
    "\n",
    "if run_model:\n",
    "\n",
    "    model = Word2Vec(data[1],min_count=1,\n",
    "                     window=5,\n",
    "                     vector_size=vector_size,\n",
    "                    #  sample=6e-5, \n",
    "                    #  alpha=0.03, \n",
    "                    #  min_alpha=0.0007, \n",
    "                    #  hs = 0,\n",
    "                    #  negative=20,\n",
    "                     workers=cores-1,\n",
    "                    #  seed = semilla,\n",
    "                     sg = 1)\n",
    "    \n",
    "    # model.save(path_model)\n",
    "\n",
    "else:\n",
    "    model = Word2Vec.load(path_model)\n",
    "\n",
    "vocab = list(model.wv.index_to_key)                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tecnologia', 0.8312691450119019),\n",
       " ('sncti', 0.8216502666473389),\n",
       " ('iccti', 0.8028162121772766),\n",
       " ('proinnovate', 0.7993803024291992),\n",
       " ('senescyt', 0.7975841760635376),\n",
       " ('micitt', 0.7975763082504272),\n",
       " ('jannixia', 0.7932529449462891),\n",
       " ('ciencia', 0.7913185954093933),\n",
       " ('emprendimiento', 0.7911211252212524),\n",
       " ('fusionando', 0.7889145016670227),\n",
       " ('ctel', 0.7845596075057983),\n",
       " ('tecnologica', 0.7810786962509155),\n",
       " ('greentech', 0.7798160910606384),\n",
       " ('macrozona', 0.7785157561302185),\n",
       " ('mincyt', 0.7744529843330383),\n",
       " ('macrotendencias', 0.7741804718971252),\n",
       " ('habilitadores', 0.7730867266654968),\n",
       " ('addotar', 0.7724708318710327),\n",
       " ('ctci', 0.77205491065979),\n",
       " ('emprendedurismo', 0.769287109375)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar('innovacion', topn=20)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_mean = matrix_by_new(df=data[1], vocab=vocab, model=model, run=True, vector_size=vector_size)\n",
    "salida_categorias = df_categoria_noticia(df_noticas=data[0],lista_temas=variables,modelo=model,vector_promedio_noticias=vector_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categoria\n",
       "alianzas          0.597509\n",
       "innovacion        0.626025\n",
       "macroeconomia     0.606658\n",
       "regulaciones      0.602943\n",
       "reputacion        0.541312\n",
       "sostenibilidad    0.617127\n",
       "Name: Similitud, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida_categorias.groupby('Categoria').mean()['Similitud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sostenibilidad    50\n",
       "reputacion        50\n",
       "regulaciones      50\n",
       "macroeconomia     50\n",
       "innovacion        50\n",
       "alianzas          50\n",
       "Name: Categoria, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida_categorias.sort_values(['Categoria','Similitud'],ascending = False, inplace=True)\n",
    "muestra = salida_categorias.groupby('Categoria').head(50)\n",
    "muestra['Categoria'].value_counts()\n",
    "# muestra.to_csv('muestra_categorias.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categoria\n",
       "alianzas          0.697760\n",
       "innovacion        0.773155\n",
       "macroeconomia     0.685679\n",
       "regulaciones      0.696711\n",
       "reputacion        0.584877\n",
       "sostenibilidad    0.749972\n",
       "Name: Similitud, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muestra.groupby('Categoria').mean()['Similitud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_muestra = pd.read_csv('muestra_categorias_original.csv',index_col='Unnamed: 0')\n",
    "original_muestra.groupby('Categoria').mean()['Similitud']\n",
    "original_muestra['val_cat'] = original_muestra['news_url_absolute'].apply(lambda x: x.split('/')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>news_url_absolute</th>\n",
       "      <th>news_init_date</th>\n",
       "      <th>news_final_date</th>\n",
       "      <th>news_title</th>\n",
       "      <th>news_text_content</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Similitud</th>\n",
       "      <th>val_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17649</th>\n",
       "      <td>news77900</td>\n",
       "      <td>https://www.letrap.com.ar/nota/2022-8-5-10-32-...</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>2022-08-14</td>\n",
       "      <td>Un ortodoxo con pasado en Lavagna, el vice ...</td>\n",
       "      <td>Con la designacion de Gabriel Rubinstein frent...</td>\n",
       "      <td>macroeconomia</td>\n",
       "      <td>0.688243</td>\n",
       "      <td>nota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>news45579</td>\n",
       "      <td>https://www.eldiariodelarepublica.com/nota/202...</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>Batakis se reunio con la titular del FMI, que ...</td>\n",
       "      <td>La ministra de Economia, Silvina Batakis, mant...</td>\n",
       "      <td>macroeconomia</td>\n",
       "      <td>0.685096</td>\n",
       "      <td>nota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>news46743</td>\n",
       "      <td>https://www.letrap.com.ar/nota/2022-7-16-17-47...</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>Entre Guzman y su sucesora, Pesce no duda: \"Es...</td>\n",
       "      <td>Las criticas que antes resonaban por los pasil...</td>\n",
       "      <td>macroeconomia</td>\n",
       "      <td>0.664930</td>\n",
       "      <td>nota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         news_id                                  news_url_absolute  \\\n",
       "17649  news77900  https://www.letrap.com.ar/nota/2022-8-5-10-32-...   \n",
       "9273   news45579  https://www.eldiariodelarepublica.com/nota/202...   \n",
       "9537   news46743  https://www.letrap.com.ar/nota/2022-7-16-17-47...   \n",
       "\n",
       "      news_init_date news_final_date  \\\n",
       "17649     2022-07-30      2022-08-14   \n",
       "9273      2022-07-15      2022-07-30   \n",
       "9537      2022-07-15      2022-07-30   \n",
       "\n",
       "                                              news_title  \\\n",
       "17649     Un ortodoxo con pasado en Lavagna, el vice ...   \n",
       "9273   Batakis se reunio con la titular del FMI, que ...   \n",
       "9537   Entre Guzman y su sucesora, Pesce no duda: \"Es...   \n",
       "\n",
       "                                       news_text_content      Categoria  \\\n",
       "17649  Con la designacion de Gabriel Rubinstein frent...  macroeconomia   \n",
       "9273   La ministra de Economia, Silvina Batakis, mant...  macroeconomia   \n",
       "9537   Las criticas que antes resonaban por los pasil...  macroeconomia   \n",
       "\n",
       "       Similitud val_cat  \n",
       "17649   0.688243    nota  \n",
       "9273    0.685096    nota  \n",
       "9537    0.664930    nota  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nota\n",
    "# 2022\n",
    "\n",
    "val = original_muestra[original_muestra['Categoria']=='macroeconomia']\n",
    "val[val['val_cat']=='nota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1550\n",
    "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [model2.wv[word] for word in data[id] if word in vocab]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in data[id] if word in vocab]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 3)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(x = tsne_df[:, 0], y = tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, df.shape[0], 10))\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias[salida_categorias['macroeconomia']>0.6].sort_values('macroeconomia',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias.groupby('Categoria').count()['news_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noti_categorias = salida_categorias[variables]\n",
    "\n",
    "corr = noti_categorias.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias['len_content'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias['len_content'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias_filtro = salida_categorias[salida_categorias['len_content']<300]\n",
    "print(salida_categorias.shape[0])\n",
    "print(salida_categorias_filtro.shape[0])\n",
    "print(salida_categorias_filtro.shape[0]/salida_categorias.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias_filtro['len_content'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_categorias_filtro[salida_categorias_filtro['len_content']==254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne no icnluido dentro del procesameinto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Again use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(X)\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "sns.scatterplot(x = tsne_df[:, 0], y = tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 800, 80)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(tsne_df[title, 0], tsne_df[title, 1], titles_list[title], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp-bancolombia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ba5a77d130743879ddd6b983fd8f274aeb4bbf8c40380f0bddb5c66d65acca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
